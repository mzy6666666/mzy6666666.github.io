---
title: RPC面试总结
date: 2024-07-08 16:23:38
tags: rpc
categories:
    项目
password: mzy666
abstract: 有东西被加密了, 请输入密码查看.
message: 您好, 这里需要密码.
wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.
wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.
---

# MPRPC面试总结

## 项目概述

![img](https://mzy777.oss-cn-hangzhou.aliyuncs.com/img/939e4b81aeb94e309a2768841e817ae1.png)

![流程框架](https://mzy777.oss-cn-hangzhou.aliyuncs.com/img/980d6c6df43e4472ac8649689dc782e9.png)

## 原理

①服务端启动的时候先扫描，将服务名称及其对应的地址(ip+port)注册到注册中心，这样客户端才能根据服务名称找到对应的服务地址。

②客户端去注册中心找服务地址，有了服务地址之后，客户端就可以通过网络请求服务端了。

②客户端调用远程方法的时候，实际会通过创建代理对象来传输网络请求。

④客户端生成request对象（类名、方法名以及相关参数）作为消息体，拼接消息头，然后通过muduo传输过去。

⑤当客户端发起请求的时候，多台服务器都可以处理这个请求。通过负载均衡选一台服务器。

⑥服务器收到客户端传过来的信息后进行解码，看看客户端需要要干什么，然后反射调用方法得到result，把result封装成rpcMessage，再传回去。

⑦客户端收到rpcMessage，解码，拿到自己想要的结果；

## 简单描述项目、实现了怎样的功能？采用了哪些技术栈

这个项目是基于C++语言实现的一个RPC分布式网络通信框架项目，使用CMake在Linux平台上构建编译环境。它可以将任何单体架构系统的本地方法调用重构为基于TCP网络通信的RPC远程方法调用。该框架实现了**同一台机器的不同进程之间或不同机器之间的服务调用**。它适用于将**单体**架构系统拆分为基于**分布式微服务调用**的部署，通过将高并发性能要求的微**服务部署多份**来提升系统整体**并发**性能，并具有模块服务**独立升级**和服务间**解耦**的优势。

> Linux 环境下基于 muduo、Protobuf 和 Zookeeper 实现的一个轻量级 RPC 框架。可以把单体架构系统的本地方法调用，重构成基于 TCP 网络通信的 RPC 远程方法调用，实现同一台机器不同进程或者不同机器之间的服务调用。

该项目的网络层基于高并发的Reactor网络模型muduo开源网络库实现。这使得对网络IO层和RPC方法调用处理层进行代码解耦变得更加容易，并且具有良好的并发性能。RPC方法调用使用protobuf进行相关数据的**序列化和反序列化**，可以直接在同构和异构系统中进行调用。微服务的服务注册、服务发现等功能是基于zookeeper实现的。**zookeeper**本身提供了C的API，可以通过API与zookeeper服务器进行通信。

> 异构系统指的是有的rpc 进程是C++写的服务，有的rpc 进程是Golang 或者Java 写的服务，但因为都是基于统一的protobuf 协议进行通信的，所以直接可以进行远程rpc 通信。

## 做这个项目的原因

两个不同的服务器上的服务提供的方法不在一个内存空间，需要网络编程才能传递方法调用所需要的参数，方法调用的结果也需要通过网络编程来接收。如果手动网络编程来实现这个调用过程的话工作量大，因为需要考虑底层传输方式(TCP 还是UDP)、序列化方式等等方面。RPC可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。

## 什么是 RPC ？

- RPC (Remote Procedure Call)即**远程过程调用**，是分布式系统常见的一种通信方法。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。
- 除 RPC 之外，常见的多系统数据交互方案还有分布式消息队列、HTTP 请求调用、数据库和分布式缓存等。
- **RPC会隐藏底层的通讯细节**（不需要直接处理Socket通讯或Http通讯）。

## 为什么要用RPC

RPC 最大的特点就是可以让我们像调用本地函数一样调用远程函数。RPC是解决分布式系统通信问题的重要技术，在搭建一个复杂的分布式系统过程中，如果开发人员想要调用某个远端函数，在编写这个函数逻辑的时候还要**编写网络通信有关**的一系列逻辑，这样非常不利于开发。所以就需要RPC框架对调用远端函数和接收远端函数的处理结果这整个过程进行封装，让开发人员在调用远端函数时能和调用本地函数一样，具备相同的语义性。

解决了单机模型和集群模型的缺点：

1. 受限于硬件资源，聊天服务器所能承受的用户的并发量
2. 任意模块的更改，都会导致整个项目代码重新编译、部署
3. 系统中，有些模块是属于CPU密集型的，有些模块是属于`I/O`密集型的，造成各模块对于硬件资源的需求是不一样的

## 在分布式系统中，不同服务之间应该如何通信呢？

在 RPC 框架中，主要是使用注册中心来实现服务注册和发现的功能。服务端节点上线后自行向注册中心注册服务列表，节点下线时需要从注册中心将节点元数据信息移除。客户端向服务端发起调用时，自己负责从注册中心获取服务端的服务列表，然后在通过负载均衡算法选择其中一个服务节点进行调用。以上是最简单直接的服务端和客户端的发布和订阅模式，不需要再借助任何中间服务器，性能损耗也是最小的。

## **微服务之间如何进行通信？**

- **单体项目**时：**一次服务调用**发生在**同一台机器**上的**同一个进程内部**，也就是说调用发生在本机内部，因此也被叫作本地方法调用。
- **微服务项目**时：**服务提供者**和**服务消费者**运行在**两台不同物理机上的不同进程内**，它们之间的调用相比于本地方法调用，可称之为远程方法调用，简称 RPC

## **RPC包含哪些部分？**

一个RPC框架要包含

- 客户端和服务端建立网络连接模块( **server**模块、**client**模块 )
- 服务端**处理请求模块**
- **协议**模块
- **序列化**和**反序列**模块。

## **设计一个RPC会考虑哪些问题？**

设计一个RPC框架，可以从PRC包含的几个模块去考虑，对每一个模块分别进行设计。

- **客户端**和**服务端如何建立网络连接**？ 
- **服务端**如何**处理请求**？ 
- **数据传输**采用什么**协议**？ 
- **数据**该如何**序列化**和**反序列化**？

## zookeeper在你的项目中起到什么作用

每个后台server 进程启动后，都会向zk 上去注册自己对外提供的rpc 服务；当调用rpc 方法的client 想要调用一个服务，会去zk 上查询下rpc 服务所在的server 节点，拉取服务所在server 的ip 地址和port 信息，然后进行远程服务调用。所在zk 在项目中主要做一些**服务注册**和**服务发现**。

服务治理的内容除了**服务注册**、**服务发现**，常见的还有**服务熔断**、服务降级等等，一般都会实现一个**统一的微服务网关**统一做这些处理，但是我这个分布式rpc 项目暂时没有涉及这么复杂的服务治理手段，像开源的大型分布式框架百度的brpc，腾讯的tars 等对于分布式治理非常详细，我通过这个项目把分布式微服务的优势，底层实现原理都搞清楚了，后续打算再研究一下上面的开源框架代码继续学习。

## 为什么推荐Zookeeper作注册中心

**zk将全量数据存储在内存中，可谓是高性能，而且支持集群，可谓高可用，另外支持事件监听。这些特点决定了zk特别适合作为注册中心(数据发布/订阅)。**

## 当服务提供者的某台服务器宕机或下线时”，zookeeper如何感知到呢？

zookeeper提供了“心跳检测”功能，**它会定时向各个服务提供者发送一个请求（实际上建立的是一个 socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除。**

## 注册中心会将新的服务IP地址列表发送给服务消费者机器是如何实现的呢？

- 主动拉取策略：服务的消费者定期调用注册中心提供的服务获取接口获取最新的服务列表并更新本地缓存，经典案例就是Eureka。

- 发布-订阅模式：服务消费者能够实时监控服务更新状态，通常采用监听器以及回调机制。Zookeeper使用的是“发布-订阅模式”。

## zookeeper 服务容灾？zookeeper 服务节点挂掉之后，怎么删除它？

容灾：在集群若干台故障后，整个集群仍然可以对外提供可用的服务。删除：**使用临时节点，会话失效，节点自动清除**。

## Zookeeper 有几种角色？

群首（leader），追随者（follower），观察者（observer）

### 注册中心对于服务端掉线时怎么处理

移出ip链表，发送给客户端，等待服务器上线，重新连接；

## CAP 理论解释下？P是什么？

- 一致性（**C**onsistency）多个副本之间的数据一致性
- 可用性（**A**vailability）在合理规定的时间内，是否能返回一个明确的结果。
- 分区容错性（**P**artition tolerance）在分区故障下，仍然可以对外提供正常的服务。

一个分布式系统在以上三个特性中：最多满足其中的两个特性。**Zookeeper满足CP**

## zookeeper的节点类型?

（持久，临时，顺序）

## 为什么需要序列化？

网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是不能直接在网络中传输的，所以我们需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程我们一般叫做“序列化”。这时，服务提供方就可以正确地从二进制数据中分割出不同的请求，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象，这个过程我们称之为“反序列化”。

## 序列化和反序列化有什么作用

（1）**实现了数据的持久化**：永久性保存对象，保存对象的字节序列到本地文件或者数据库中；
（2）**序列化实现远程通信**：通过序列化以字节流的形式使对象在网络中进行传递和接收；
（3）通过序列化在进程间传递对象；

## 为什么使用Protobuf而不是Json

- JSON 进行序列化的额外空间开销比较大，对于大数据量服务这意味着需要巨大的内存和磁盘开销；
- 序列化后体积相比 JSON、Hessian 小很多;
- 序列化反序列化速度很快，不需要通过反射获取类型；

json是纯文本操作，protobuf是基于二进制序列化数据的，进行rpc通信时数据占用带宽比json效率高很多，其次protofbuf 原生提供了基于rpc service 方法调用代码框架，封装了很好用的接口，实现rpc 方法远程调用非常简单，我只需要关注服务管理和rpc 通信流程的开发。而json 只是一个单纯的数

据序列化和反序列化协议，没有protobuf 上面说的这些额外的功能，所以我项目上选择了protobuf 作为rpc 数据通信协议。

## 项目中采用线程同步解决了哪些问题？

微服务进程启动后，在代码上需要调用zoo_create 方法链接zookeeper server，但是全局的watcher 是在另外一个线程中工作的，zookeeper server 连接成功是通过watcher 回调来通知应用程序的，所以在这里我用了线程间的sem信号量同步等待watcher 中响应zk 连接成功后，程序再继续执行下面的操作。


### 为什么用 RPC，不用 HTTP

使用 TCP 和使用 HTTP 各有优势：

**传输效率**：

- TCP，通常自定义上层协议，可以让请求报文体积更小 
- HTTP：如果是基于HTTP 1.1 的协议，请求中会包含很多无用的内容 

**性能消耗**，主要在于序列化和反序列化的耗时

- TCP，可以基于各种序列化框架进行，效率比较高 
- HTTP，大部分是通过 json 来实现的，字节大小和序列化耗时都要更消耗性能 

**跨平台**：

- TCP：通常要求客户端和服务器为统一平台 
- HTTP：可以在各种异构系统上运行 

**总结**：
  RPC 的 TCP 方式主要用于公司内部的服务调用，性能消耗低，传输效率高。HTTP主要用于对外的异构环境，浏览器接口调用，APP接口调用，第三方接口调用等。

良好的 rpc 调用是 面向服务的封装，针对服务的 可用性 和 效率 等都做了优化。单纯使用http调用则缺少了这些特性

rpc是基于长链接，不必每次通信都要像http一样去3次握手什么的，减少了网络开销；

**效率高**：RPC通常基于二进制协议，而不是像HTTP那样基于文本协议，因此通常会更快

RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。

rpc是一种概念，http也是rpc实现的一种方式

http接口是在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；优点就是简单、直接、开发方便。利用现成的http协议进行传输。但是如果是一个大型的网站，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http一样去3次握手什么的，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。第三个来说就是安全性。最后就是最近流行的服务化架构、服务化治理，RPC框架是一个强力的支撑

## 项目有什么需要改进的地方吗？

我目前把主要精力放在了分布式rpc 框架设计和服务调用的实现细节上了，暂时对远程服务调用没有做压力测试，后续计划做用专业的Jmeter 或者LoadRunner 工具做一下压力测试，看能否找到一些瓶颈以及在代码上可以优化的地方

我打算通过学习更多开源的分布式服务治理框架，在我的分布式框架项目上集成更多功能，比如我在实现的分布式部署框架上内嵌一个分布式服务治理网关，实现服务流量监控、调用监控、日志、服务熔断降级等功能

## 你的项目优势在哪里？

后台服务设计部署主要就三种类型，**单体、集群和分布式**微服务，我之前做过基于nginx负载均衡的集群聊天服务器项目，对于分布式系统的设计优势以及通信方式不太了解。但是对于集群类的项目，只是在服务器的数量上进行扩展，存在很多的限制，修改模块项目代码还是要重新编译，而且要进行多次编译。比如后台管理不需要高并发，所以不需要每台机器都部署。

我之前只知道简单的epoll + 多线程的编程模型，没有接触过开源的高并发网络库，这个项目中使用了muduo 网络库，是基于C++实现的，它的好处是one loop per thread，我甚至还阅读了一下muduo 网络库的源码，并用C++11 实现了它的核心网络功能…

通过这个项目对于protobuf 也知道它的应用场景和优势了,它提供了**基于rpc service 方法调用代码框架**，封装了很好用的接口，之前只用过json。另外对于服务器中间件zookeeper 也学习了很多知识，还有服务注册、服务发现等分布式微服务必备的一些服务治理等等。


## 在一个局域网有A和BCA和B之间是不知道互相的IP的A给B主机传一个16的文件;解决方案

## 假如需要用json作为数据传输序列，你的项目哪里需要改动，怎么改动呢？

在数据序列化和反序列化方面可以直接修改成json 处理的代码即可。

但是json 没有办法实现protobuf 的service rpc 方法的定义，无法生成特有的封装好的rpc 方法调用的代码架构，json 根本就没有那一套机制，它只是简单的序列化和反序列化协议，json 没有什么配置文件，可以像protobuf 那样生成一套封装好的rpc 代码调用架构。

那么protobuf 生成的这一套rpc封装的代码逻辑如果换成json 的话，需要自己去实现一下，增加了比较大的工作量。

另外json 发送的都是文本数据，而protobuf 传输的是二进制数据，所以在解决tcp 粘包问题的代码上，也需要进行相应的修改。

## 假如我现在需要你的框架可以接受的并不是一个特定类型的序列，比如我客户端传输json，有protobuf等很多个序列，那么你该怎么做让你的框架同时支持这些序列技术呢？就是怎么判断收到的数据是protobuf 还是 json？有没有什么好的办法呢？

首先，考虑将代码中RpcService高度抽象出来，作为一个基类，基于多态实现不同的数据协议解析派生类，不同派生类都重写虚函数，分别处理各自负责的json 和 protobuf的解析，代码上的接口可基于基类指针或引用进行使用,后续就是增加新的协议处理，只需要添加新的派生类即可，rpc处理的相关代码逻辑都是基于基类处理的，不用做改动.

上面只是说在代码设计上的变化，至于框架上怎么区分具体的协议是json 还是 protobuf,主要方法有两种：

1.再增加一层抽象的数据协议，里面可携带不同数据类型，包括json的、protobuf的、可在抽象的数据协议里面添加一个字段表示协议类型方便判断框架识别具体是哪个协议

2.RpcServer端可以监听两个端口，一个端口专门收发处理json数据，另一个端口专门收发处理protobuf数据对于框架这个概念，你觉得在你的项目中，具体体现在了哪一块，或者哪个类的哪个函数？
具体体现了在我这个框架对外提供了两个基础类RpcClient和RpcServer，任何模块要实现成提供分布式rpc服务的方式，都可以启动一个RpcServer，并注册相关的Service微服务方法到中间件上，当然这一套框架都是框架内部处理的，用户只需要使用注册方法即可；

RpcClient可以像调用一个本地方法一样调用一个远程的Rpc服务，查询服务中间件zookeeper查询到的具体提供服务的主机的ip和port信息，然后发起一个标准的rpc调用，包括参数的组装，请求调用的发起；

RpcServer端收到请求会反序列化调用参数和方法名称，执行方法调用，再返回响应等，这一套都在框架内部实现完了，对于调用者的使用感受来说，和调用一个本地方法没太大差别，因为具体的rpc调用细节都是框架处理完了。

## 你的项目设置了一个IO线程和3个工作线程，那么者几个线程具体怎么进行协调运行呢？

muduo库是一个典型的Reactor 网络模型的实现，IO 线程主要负责监听新连接，通过事先创建的evenfd 来做IO 线程和工作线程之间的通信工作（当然实际过程中还可以用socketpair 做双向通信唤醒），IO 线程根据负载算法（这里muduo 采用轮询算法）把新连接的socketfd 通过evenfd 唤醒相应的工作线程，并注册到工作线程的epoll 上继续处理后续的数据读写事件。而且这里也不一定3 个工作线程，一般根据当前CPU 的核心数量来确定就可以，C++11 体提供了thread::hardware_concurrency()可以获取CPU 核心数量。

## 如果要实现一个全部是异步模式的RPC框架，应该怎么做？

我现在实现的这个分布式rpc 框架确实是同步的，因为rpc 请求的调用结果响应并没有什么通知机制（同步和异步的概念原理对比在手写muduo 网络库课程中详细的做了说明讲解），是一直等待rpcserver 返回响应结果后再继续执行的，所以是一个典型的同步rpc 通信框架。

如果要实现一个异步的rpc 框架，也就是说rpcclient 发起rpc 请求后，就可以做其它逻辑处理，而不用一直死等rpc 接口响应后才继续执行，这样的话就需要一个异步回调通知的机制了（扩展：nodejs全部都是这样的异步回调机制，所以可以做到单线程也能高并发），具体的实现就需要更改框架的设计，这里我可以实现两种不同的模式：

模式一：当应用层调用一个rpc 接口，框架把该rpc 接口调用封装成一个任务，并给这个任务分配一个taskid（或者叫rpcid 也行，名字任由你起），放到rpc 框架的一个任务队列中，当然需要把这个taskid返回给应用层（后面还要拿这个taskid 去异步查询结果的），这时应用层就可以去做其它事情，由rpc框架从任务队列中取rpc 任务发起请求，rpcserver 处理完成后进行响应（当然通信交互中都会携带这个taskid），rpc 框架会把响应结果放在另外一个存储rpc 响应结果的队列中，应用程序在合适的时候通过taskid 查询之前rpc 接口调用的结果，实现异步的rpc 通信机制。

模式二：当应用层调用一个rpc 接口，不仅传入rpc 接口调用所需要的参数，还需要传入一个回调函数（C 或者C++普通函数，或者lambda 表达式都可以），剩下的和模式一的流程基本是一样的这里省略若干个字，当rpc 框架最后接收到这个请求的响应后，根据taskid 找到该任务对应的回调函数进行调用（当然这里实现的时候就可以用一个map 存储taskid=>回调函数的映射关系了），这样应用层相当于就得到了通知并执行了相应的回调函数，实现了异步的rpc 通信机制（nodejs 的接口调用异步通知机制）。

## rpc服务接口调用出错了怎么办

首先这里提到的rpc 服务调用接口出错了，得分两种情况来看待：

rpc 服务接口调用响应成功了，但是返回值的errcode 却不是0 导致的调用结果出现错误。这种是由于业务逻辑执行有问题而导致的接口出错，是程序正常逻辑的表现，比如调用login 这个rpc 接口，但是传入的用户名或者密码不正确，那么rpc 接口调用成功了，但是返回值携带了错误信息，可以提示用户重新输入用户名或者密码等。
因为rpc 服务接口调用本质上还是一个tcp 网络通信，所以在网络环境比较差的情况下，调用过程中出错的可能性还是很大的。如果框架上要考虑rpc 调用的容错机制，增加rpc 框架服务调用的高可用性，在框架实现的时候可以增加更多的逻辑实现来保障可靠性。比如在框架中rpc 发起调用请求后，recv调用rpc 的响应结果时给socket 设置超时时间，那么如果recv 的返回值是-1（出错）或者0（连接断开），或者是由于超时timeout 返回，那么可以从zookeeper 上重新去搜索一下当前rpc服务还有哪个主机可以提供，再重新选择一个主机发起rpc 访问

## rpc服务接口调用的时候，服务端没有响应怎么办

这种问题可以在框架上发起rpc 请求的socket 设置SO_RCVTIMEO 的超时时间（默认的recv 是没有收到响应，也没有受到服务器的FIN 挥手报文的话，recv 会一直阻塞下去），也就是指定的timeout时间内没有收到rpc 请求的响应，判断当前rpc 请求失败，此处为了实现高可用性，可以再尝试请求比如3 次，3 次都失败的话，可以从zookeeper 上查询还有没有其它主机提供同样的rpc 服务，然后重新向新主机发起请求，直到请求成功，如果所有提供该rpc 服务的主机都没有响应或已经没有其它主机提供该rpc 服务了，那么尝试3 次后，可以判定请求失败（确实有其它因素导致所有的服务器都挂掉了，这一般是严重的线上bug）。

# rpc在并发量较大的情况下会出现error

没对消息进行粘包处理

1. 在消息前面在加个消息长度
2. 添加帧界定符

## 微服务网关了解吗？有什么作用？

当我把一个单体服务应用程序重构程分布式微服务，那么原来在客户端调用一个接口就能完成的业务现在客户端就要调用多个微服务接口才能完成业务处理，为了不增加客户端使用的复杂度，一般客户端都会和微服务网关直接通信，微服务网关相当于站在客户端对面，背后管理所有的微服务，客户端还是发起一个请求到微服务网关，由网关来负责调用该业务相关的所有其它微服务接口，最后统一给客户端返回，微服务网关本身也是一个微服务组件。

对于所有的rpc 微服务一般都会有一些通用的处理操作，比如rpc 服务调用的身份鉴权（检查token 是否合法，有没有过期等）、限流、降级、rpc 接口调用监控、日志等等，不可能在每一个rpc 服务中都实现一套这样的处理机制，因为实现起来这些代码都一样，还存在大量的重复，所以一般rpc 请求都是先经过微服务网关，由网关来做上面的通用处理操作，符合网关设置的rpc 访问规则，才会放行真正的去请求相应的rpc 服务，如果网关检查不符合规则，就会拒绝掉当前的rpc 请求。

## 项目中遇到的难题

首先关于zookeeper 的使用和protobuf 的使用，也是自己查阅资料做出了很大的努力。zookeeper 的全局watcher 功能这一块，一开始也并没有意识到需要同步的操作，后来也是翻阅资料解决了这个问题。当然我在项目中也收获了其他的知识，比如我在使用bind 绑定器绑定一个函数对象的时候，用const 和不用const 修饰的两个函数实际上是构成重载的，所以一开始忽略了这一点，在后面经过不断地编译调试以后，解决了这些问题。

实现项目的过程中，出现rpc 请求方发送的数据没有问题，但是rpc 服务方法接收方接收到的数据不全，通过加关键日志打印和gdb 调试项目代码等，最终发现了问题，是发送方用了strlen，而没有写protobuf 正确的数据长度size

我的项目中，组织了protobuf 里面记录个各个数据段的长度，但是在接收方并没有进行tcp 粘包代码处理逻辑的实现，这里也是一个问题点可以修改代码改进

## 你对这些服务器中间件有更深层的了解吗？你还了解他的哪些功能？

服务器中间件我了解过 zookeeper、consul、etcd（k8s底层就是用etcd做服务管理） 都可以完成常见的服务治理功能，包括服务注册、服务发现等，我的这个项目里主要通过zookeeper来完成这个功能，它的API接口操作起来方便，还实现了watcher回调功能，原生提供了心跳检测动态监听server的上下线。

当然除了服务治理，我了解到的zk 还可以支持分布式系统全局的一些统一公共配置信息，因为它是基于znode 实现的，和linux 的文件系统非常相似，分布式系统的各个服务可以通过读取zk 节点拉取统一配置信息。

zk还可以实现分布式锁，对于不同服务之间可以做一些跨服务的同步操作，使用起来方便。

zk还可以实现集群中的主备切换功能，可以应用在容灾的场景，master主机挂掉能否实现slave备机工作，主机恢复正常能够再切换回来提供服务。

## 负载均衡

系统中的某个服务的访问量大，将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。如何正确选择处理该请求的服务器就很关键。负载均衡为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题。

## 一致性哈希

【原理】hash空间组成一个虚拟的圆环，将各个服务器使用 Hash 函数进行哈希，可以选择服务器的IP或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置。将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器。

【优点】对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。

## dubbo的负载均衡算法

①RandomLoadBalance：根据权重随机选择（对加权随机算法的实现）；

②LeastActiveLoadBalance：最小活跃数负载均衡。初始状态下所有服务提供者的活跃数均为 0（每个服务提供者的中特定方法都对应一个活跃数），每收到一个请求后，对应的服务提供者的活跃数 +1，当这个请求处理完之后，活跃数 -1。

Dubbo 就认为谁的活跃数越少，谁的处理速度就越快，性能也越好，这样的话，我就优先把请求给活跃数少的服务提供者处理；

③ConsistentHashLoadBalance：一致性哈希；

④RoundRobinLoadBalance：加权轮询负载均衡，加权轮询就是在轮询的基础上，让更多的请求落到权重更大的服务提供者上。