---
title: muduo面试相关
date: 2024-07-04 16:51:47
tags: muduo
categories:
    项目
password: mzy666
abstract: 有东西被加密了, 请输入密码查看.
message: 您好, 这里需要密码.
wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.
wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.
---
**阻塞IO非阻塞io异步io同步io那些，回调函数作用**

> 阻塞IO：在阻塞 IO 模式下，系统调用（如 `read`、`write`）会一直阻塞，直到操作完成。对于 `read` 调用，阻塞意味着要等到有数据可读；对于 `write` 调用，阻塞意味着要等到数据完全写入。
>
> 非阻塞IO：在非阻塞 IO 模式下，系统调用立即返回。如果操作不能立即完成，调用将返回一个错误（通常是 `EAGAIN` 或 `EWOULDBLOCK`）。非阻塞 IO 通常与轮询或事件驱动机制结合使用，以避免阻塞调用。
>
> 异步IO：异步 IO 允许应用程序发出 IO 请求后立即返回，并在**操作完成时**通过回调或通知机制告知应用程序。异步 IO 可以避免阻塞线程，提升并发性能。
>
> 同步IO：同步 IO 通常意味着调用线程要等待 IO 操作完成。这种模式包括阻塞 IO 和非阻塞 IO（结合轮询或事件驱动）。

我看你这个muduo用了LRU和二叉堆作为定时器，那你可以讲讲吗？

> 我答得是我这是跟`libevent`的定时器一样，有两种策略，**LRU适用于定时时间相同的**，二叉堆适用时间不同的，又讲了下LRU内部的实现机制，自己也写了个二叉堆最后还是用的优先级队列
>
> 

那你能介绍一下`muduo`库中是线程是如何通信的？进程中要怎么通信？

> `muduo`跨线程通信主要通过`EventLoop`的成员函数`runInLoop`实现，每个IO线程都对应了一个`EventLoop`的指针，通过判断`loop`指针所指线程与调用`runInLoop`函数的线程，如果不是同一个线程，就先把要跨线程调用的函数存到一个任务队列里面，然后再执行一个`wakeup`函数来唤醒线程，其实就是向`wakeuoFd_`里面写8字节数据，这就对导致原来阻塞监听的poll函数返回活跃通道，这样被唤醒的线程就能取执行之前注册的回调函数。
>
> `muduo`库实现跨线程调用其实就是通过向指定的线程的`wakeupFd_`(是通过`eventfd`创建的文件描述符)上随便发点数据，让对应线程中`loop()`循环中之前阻塞监听的`poll`函数返回活跃通道，好让线程赶快执行回调函数。这样，回调函数执行完毕，其就可以调用`doPendingFunctors`函数执行我们之前添加的回调函数了
> **`eventfd`是Linux特有的（Linux 2.6以后），专用于事件通知的机制，类似于管道**
>
> **`eventfd`是一个比`pipe`更高效的线程间事件通知机制，一方面它比`pipe`少用一个fd，节省了资源；另一方面，`eventfd`的缓冲区管理也简单得多，全部`buffer`一共只有8字节，不像`pipe`那样可能有不定长的真正`buffer`。**
>
> `mainloop`通常会通过`setthreadnumber`创建子线程，当有新连接到来时，主线程会把连接分配给子线程，并事先注册回调，等待`wakeup`唤醒子线程

为什么看muduo库，而不是grpc、libevent库，libevent有了解过吗？

> 事件在 muduo 中包括 Socket 可读写事件、定时器事件。在其他网络库中如 libevent 也包括了 signal、用户自定义事件等。
>
> 负责事件循环的部分在 muduo 被命名为 `EventLoop`，这个命名也基本是个约定语了，其他库如 Netty、libevent 也都有对应的组件。

muduo库的整体框架？画一下它们类的UML图，解释这个类有什么用？ 

muduo的定时器是怎么实现的？ 

> muduo 的定时器实现主要通过 `timerfd` 结合事件循环机制实现精准的定时事件管理。`timerfd` 提供了一个文件描述符，用于通知定时事件，适合与 epoll 等 I/O 多路复用机制一起使用。muduo 使用红黑树（`std::set`）来管理定时器，以保证高效的插入和删除操作。

timefd是什么，有什么用？

> `timerfd` 是 Linux 提供的一种定时器接口，用于创建文件描述符，以通知定时器事件。它的主要优点是与 epoll 和 select 等 I/O 多路复用接口的良好集成，使其非常适合在事件驱动编程中使用。

为什么选择timerfd实现定时器

> 1. `sleep / alarm / usleep`在实现时有可能使用了SIGALRM信号，多线程程序中尽量避免使用信号，因为处理起来比较麻烦（信号通知进程，所有线程都将接收到这个信号，谁处理好）。另外，如果网络库定义了信号处理函数，用户代码(main函数等使用库的程序)也定义了信号处理函数，这不就冲突了，该调用哪个好
> 2. `nanosleep / clock_nanosleep`是线程安全的，但是会让当前线程挂起等待一段时间，这会导致线程失去响应。在非阻塞网络编程中，绝对不能让线程挂起的方式来等待一段事件。正确的做法是注册一个事件回调函数
> 3. `gettimer`和`timer_create`也是用信号来传递超时信息，在多线程中程序中也会有麻烦
> 4. `timerfd_create`把时间变成了一个文件描述符，该描述符在定时器超时的那一刻变为可读，可以很方便的融入到select/poll/epoll中，用同一的方式来处理IO事件和超时事件

**看你项目用到muduo库也刨析过源码，简单的介绍一下muduo库，还有项目遇到的困难**

> Muduo网络库基于Reactor模式，实现了高性能的事件驱动编程。其核心组件包括EventLoop、TcpServer、TcpConnection等，分别负责事件循环、服务器管理和连接处理。Muduo库的设计目标是简单、高效和稳定，它提供了丰富的API接口，使得[开发者](https://cloud.baidu.com/product/xly.html)能够快速地构建出高性能的网络应用。

> Muduo网络库基于Reactor模式，实现了高性能的事件驱动编程。muduo的网络设计核心为一个线程一个事件循环，有一个main Reactor负载accept连接，然后把连接分发到某个sub Reactor(采用轮询的方式来选择sub Reactor)，该连接的所用操作都在那个sub Reactor所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用CPU，Reactor poll的大小是固定的，根据CPU的数目确定。如果有过多的耗费CPU I/O的计算任务，可以提交到创建的ThreadPool线程池中专门处理耗时的计算任务。

> muduo是一个基于非阻塞IO和事件驱动的C++高并发TCP网络库，使用的**线程模型是one loop per thread**，所谓one loop per thread指的是：
>
> - 一个线程只能有一个事件循环（EventLoop）
> - 一个文件描述符（file descriptor，通常简称fd）只能由一个线程进行读写，换句话说就是一个TCP连接必须归属于某个EventLoop管理。但返过来不一样，一个线程却可以管理多个fd。
>
> 
>
> **EventLoop事件循环**
>
> EventLoop是Muduo网络库的核心组件之一，负责事件循环和事件处理。它采用了基于epoll或kqueue的高效事件通知机制，实现了对IO事件、定时器事件和信号事件的统一处理。EventLoop的设计使得开发者可以方便地将各种事件处理逻辑绑定到事件循环中，实现异步非阻塞的网络编程。
>
> **TcpServer服务器管理**
>
> TcpServer是Muduo网络库中用于管理TCP服务器组件的类。它封装了服务器的创建、启动、停止等操作，提供了简洁的API接口供开发者使用。TcpServer内部使用了多线程模型，通过线程池实现了对多个连接的高效处理。同时，它还支持多种连接处理方式，如单线程处理、多线程处理等，以满足不同场景下的需求。
>
> **TcpConnection连接处理**
>
> TcpConnection是Muduo网络库中用于处理TCP连接的类。它负责连接的建立、断开以及数据的读写等操作。TcpConnection通过回调函数机制，使得开发者可以灵活地处理各种连接事件，如连接建立、数据到达、连接断开等。此外，它还提供了丰富的API接口，用于获取连接状态、发送数据等操作。

**为什么要选one loop per thread模型**

> 如果一个TCP链接在多个线程中处理，会出现如下情况：
>
> 1. socket被意外关闭。A线程要从socket中读/写消息，但是该socket被B线程给close了，更糟的情况是，B close后，新的连接的socket刚好使用的是B关闭的socket，那A线程再次进行读/写早已经不是原来的那个连接了（这种现象叫串话）
> 2. 不考虑关闭，只考虑读写也有问题。比如A、B线程同时对一个TCP连接进行读操作，如果两个线程几乎同时各自收到一部分消息，那如何把数据拼接成完整的消息，如何知道哪部分数据先到达。如果同时写一个socket，每个线程只发送半条消息，接收方又该怎么处理，如果加锁，那还不如直接就让一个线程处理算了

**one loop per thread优点**：

> 1. 线程数目基本固定，不用频繁创建或者销毁线程
> 2. 可以很方便的在各个线程之间进行负载调配
> 3. IO事件发生的线程基本是固定不变的，不必考虑TCP连接事件的并发（即fd读写都是在同一个线程进行的，不是A线程处理写事件B线程处理读事件）

**muduo采用的多Reactor结构**

> 在muduo中，每一个线程都可以看作是一个Reactor，主线程（主Reactor）只负责监听接收新连接，并将接收的连接对应的fd分发到子Reactor中，子线程（子Reactor）就负责处理主线程分发给自己的fd上的事件，比如fd上发生的可读、可写、错误等事件，另外从fd上读取数据后，要进行的业务逻辑也是由子线程负责的。

**nginx网络模型讲一下？“一个进程一个事件循环“为什么要用多进程？和muduo的区别？**

> Nginx 是一个高性能的 HTTP 服务器和反向代理服务器，其网络模型基于异步、非阻塞的事件驱动机制。Nginx 使用了多进程架构，每个进程都拥有一个独立的事件循环，用于处理客户端请求。
>
> 主要组件
>
> 1. **主进程（Master Process）**:
>    - 负责读取和解析配置文件、管理工作进程、响应外部信号（如重启、平滑升级等）。
> 2. **工作进程（Worker Processes）**:
>    - 每个工作进程独立处理客户端请求。Nginx 采用了一个事件驱动模型，每个工作进程都有一个事件循环，用于处理所有网络事件。
>
> ### 多进程架构的优势
>
> 1. **稳定性和可靠性**:
>    - 多进程架构使得每个工作进程独立运行，互不影响。如果一个工作进程崩溃，不会影响其他进程，主进程可以快速重启崩溃的工作进程，保证服务的持续可用。
> 2. **资源隔离**:
>    - 由于每个进程有独立的内存空间，不同工作进程之间的数据不会相互干扰，减少了共享数据引发的并发问题。
>
> nginx 要保证它的高可用 高可靠性, 如果nginx 使用了多线程的时候，由于线程之间是共享同一个地址空间，当某一个第三方模块引发了一个地址空间导致的断错时 (eg: 地址越界)，会导致整个nginx全部挂掉； 当采用多进程来实现时, 往往不会出现这个问题。
>
> ### Nginx 与 Muduo 的区别
>
> 1. **架构模型**:
>    - **Nginx**: 多进程模型，每个进程独立运行，拥有自己的事件循环。
>    - **Muduo**: 多线程模型，通常使用一个主线程管理事件循环，多个工作线程处理任务。
> 2. **事件驱动机制**:
>    - **Nginx**: 使用 epoll 等 I/O 多路复用技术，每个工作进程都有一个独立的事件循环。
>    - **Muduo**: 也使用 epoll 等 I/O 多路复用技术，但通常有一个主线程负责管理事件循环，多个工作线程处理 I/O 事件和其他任务。
> 3. **进程间通信**:
>    - **Nginx**: 进程间通信通过共享内存、管道或其他 IPC 机制实现，主进程负责协调工作进程。
>    - **Muduo**: 线程间通信通过条件变量、互斥锁等线程同步原语实现。
> 4. **稳定性与扩展性**:
>    - **Nginx**: 多进程模型在稳定性和容错性上更强，适合需要高可用性、高稳定性的场景。
>    - **Muduo**: 多线程模型在资源利用和并发处理上更高效，适合高并发、低延迟的网络应用。

讲一下你对C++语言的看法

> 优点：能直接访问和修改内存，性能比较高，适合游戏开发、实时系统
>
> 缺点：复杂性也较高，直接操作内存，一不小心就内存越界

薪资

>  在不知道公司的薪资结构和范围前无法开口

rpc项目的日志模块怎么实现的？    

> 

 你用了开源的muduo库大致介绍一下（简单说了网络模型）   

>  

**什么是主从reactor模型**    

> 主从Reactor是一种基于多线程的编程模型，它通过将处理连接事件和读写事件的职责分配给不同的Reactor来实现并发处理。这种模型能够显著提高系统的并发性能和响应速度，尤其适用于处理大量并发连接和高吞吐量的场景。在主从Reactor模型中，主Reactor负责监听和处理连接事件，而从Reactor负责监听和处理读写事件。
>
> 主从Reactor模型的优点主要体现在以下几个方面：
>
> 1. 高效并发处理：通过将连接事件和读写事件分发给不同的Reactor处理，主从Reactor能够同时处理多个连接，提高了系统的并发性能和响应速度。
> 2. 事件处理分离：主从Reactor模型将连接事件和读写事件的监听和处理分离，使得事件处理更加清晰和模块化，降低了代码的复杂度。
> 3. 异步非阻塞：主从Reactor模型采用异步非阻塞的方式处理事件，避免了传统多线程模型中的线程上下文切换和阻塞问题，进一步提高了系统的性能和响应速度。
> 4. 资源复用：主从Reactor模型通过共享资源的方式实现了资源的复用，避免了资源的浪费，降低了系统的开销

回答说主线程reactor负责建立连接，然后分发连接给子线程reactor，那会带来哪些问题？    

> 惊群效应      

你做的rpc项目是为了完成业务吗    

>  回答只是简单模拟了一下聊天的业务      

序列化协议为什么用`protobuf`？   

>  **1. 效率和性能：** Protobuf是一种高效的二进制序列化格式，相比于其他文本格式（如JSON和XML），它具有更小的数据体积和更快的序列化/反序列化速度。这使得Protobuf在网络通信和数据存储方面表现出色，特别适合传输大量数据或需要高性能的场景。
>
>  **2. 跨语言支持：** Protobuf支持多种编程语言，包括C++、Java、Python等。通过定义通用的消息格式和服务接口，不同编程语言的应用程序可以相互通信和交换数据，实现跨平台和跨语言的互操作性。
>
>  **3. 数据版本控制：** Protobuf支持在数据结构发生变化时进行向前和向后兼容的数据版本控制。通过定义消息的字段编号而非字段名称，可以避免在数据结构演化时出现命名冲突或解析错误。这使得在应用程序升级和数据迁移时更加灵活和可靠。
>
>  **4. 紧凑的数据格式：** Protobuf使用二进制编码，将数据紧凑地表示为字节序列。相比于文本格式，二进制编码占用更少的存储空间，减少了网络传输的带宽消耗，并提高了数据传输的效率。
>
>  **5. 自动生成代码：** Protobuf使用定义数据结构的.proto文件，可以自动生成与编程语言相关的代码，包括消息类、序列化和反序列化方法等。这简化了开发过程，减少了手动编写和维护序列化代码的工作量。
>
>  **6. 可扩展性：** Protobuf支持向已定义的消息结构中添加新字段，而不会破坏已有的解析逻辑。这种可扩展性使得应用程序可以逐步演化和升级，而无需对整个数据结构进行全面修改。

**muduo采用固定线程的线程池有什么缺点**     

> **难以应对突发流量**: 固定线程池的大小是预先设定的，无法动态调整。当突发流量出现时，线程池可能无法快速扩展来处理额外的负载，导致响应时间增加。
>
> **资源浪费**: 在负载较低时，固定线程池中的线程仍然会占用系统资源（如内存和 CPU 时间片），导致资源浪费。

聊天项目nginx你用了之后掌握了多少，有没有看专门书籍和源码    

>  我回答了知道正向代理，反向代理，如何配置tcp负载均衡文件，怎么解决惊群效应。

**什么是惊群效应**

> 惊群现象就是多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只可能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群。

**怎么避免惊群效应**

> nginx

项目中用到`protobuf`，`protobuf`序列化后的底层数据格式是什么样（二进制格式，字段标识，长度前缀等等）

> 

muduo网络库底层处理I/O连接用到的接口

> 

先大致讲了下epoll的流程还有关键的数据结构

> 1. 创建`epoll`实例：调用`epoll_create`创建一个`epoll`文件描述符
> 2. 向`epoll`实例中添加、删除、修改事件：使用`epoll_ctl`函数向`epoll`实例中添加、修改或删除事件
> 3. 等待事件：调用`epoll_wait`函数等待事件的发生，`epoll_wait`会阻塞进程，直到有事件发生或超时
> 4. 处理事件：遍历`epoll_wait`返回的事件数组，对每个发送事件的文件描述符进行处理
>
> **关键的数据结构**
>
> **红黑树**: 用于存储所有注册的文件描述符，支持快速的插入、删除和查找操作。红黑树是平衡二叉树的一种，保证了插入、删除和查找操作的时间复杂度为 O(log N)。
>
> **双向链表**: 用于管理就绪事件，支持快速的插入和删除操作。双向链表的时间复杂度为 O(1)。

 epoll红黑树上存储的k-v分别是什么，k是fd，v是epitem

> 红黑树上的键是文件描述符`fd`，值是`epitem`结构体，存储了注册的事件类型和用户数据以及文件描述符

muduo 的设计思路（围绕 Channel、Poller、EventLoop）

> Channel：是对文件描述符和感兴趣事件的封装
>
> Poller：是对`epoll`的封装，封装了`epoll`的各个方法
>
> 由`EventLoop`通过`loop`函数从而调用`Poller`的`poll`函数得到当前就绪的IO事件返回给`EventLoop`，`EventLoop`通过调用对应`Channel`的`handleEvent`进行事件处理，周而复始。

muduo 网络库项目介绍一下，Reactor 模型具体如何处理事件

> muduo采用的是主从reactor+线程池的形式，主线程主要用于监听新的连接，在accept之后就会将这个连接分配到子线程，由子线程负责连接的事件处理（读、写等）

EventLoop 所在线程同时处理读写回调效率不低吗？

> 虽然 `EventLoop` 所在线程同时处理读写回调看似在单线程中完成，但通过非阻塞 I/O、事件驱动机制、高效的数据结构和回调机制，能够高效地处理大量并发事件和任务。在实际使用中，这种设计被证明是高效且可靠的，适用于许多高性能网络应用场景。

et和lt的具体实现

> 水平触发：只要文件描述符上的数据可以被处理，内核就会不断通知应用程序。
>
> 边缘触发：只有在文件描述符状态发生变化时（如新数据到来），内核才会通知应用程序。

讲了下muduo库的定时器队列，问我这里用map和最小堆哪个效率高

> **最小堆** 更适合于定时器事件频繁触发（即删除最小元素）的场景，因为查找和删除最小元素都是 `O(log N)`

讲了muduo库的线程异步唤醒机制 对象管理

> `if (!isInLoopThread() || callingPendingFunctors_)`
>
> 可以理解为当前IO线程阻塞于返回活跃事件的`poll()`当中，需要往`wakeupFd_`里写东西，`poll()`会立即返回，从而返回活跃事件退出阻塞。
>
> 可以理解为当前IO线程正在执行这些额外任务。为了避免任务执行完后又阻塞在了`poll()`处，就让`wakeupFd_`可读，以免任务被延迟执行。

RPC 和GRPC相比优势

> 

为什么选择`Zookeeper `

> `tinyrpc`的注册中心可以选`Zookeeper，memcached，redis`等。为什么选择`Zookeeper`，因为它的功能特性咯~
>
> - 命名服务，服务提供者向`Zookeeper`指定节点写入url，完成服务发布。
> - 负载均衡，注册中心的承载能力有限，而Zookeeper集群配合web应用很容易达到负载均衡。
> - zk支持监听事件，特别适合发布/订阅的场景，dubbo的生产者和消费者就类似这场景。
> - 数据模型简单，数据存在内存，可谓高性能

> 
>
> - 使用`ZooKeeper`作为**「`tinyrpc`的注册中心」**，使用`ZooKeeper`实现**「分布式锁」**
>
> - `ZooKeeper`，它是一个开放源码的**「分布式协调服务」**，它是一个集群的管理者，它将简单易用的接口提供给用户。
> - 可以基于`Zookeeper `实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列**「等功能」**
> - `Zookeeper `的**「用途」**：命名服务、配置管理、集群管理、分布式锁、队列管理

说下什么是`Zookeeper `的命名服务

> 命名服务是指通过**「指定的名字」**来获取资源或者服务地址。Zookeeper可以创建一个**「全局唯一的路径」**，这个路径就可以作为一个名字。被命名的实体可以是**「集群中的机器，服务的地址，或者是远程的对象」**等。一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息等。

**集群管理**

> 集群管理包括集群监控和集群控制，其实就是监控集群机器状态，剔除机器和加入机器。zookeeper可以方便集群机器的管理，它可以实时监控`znode`节点的变化，一旦发现有机器挂了，该机器就会与zk断开连接，对用的临时目录节点会被删除，其他所有机器都收到通知。新机器加入也是类似酱紫，所有机器收到通知：有新兄弟目录加入啦。

function和函数指针的区别

> 函数指针是指向函数的指针变量。它可以直接指向一个函数，也可以指向类的静态成员函数。函数指针的声明和使用相对较为简单，可以通过函数指针调用相应的函数或静态成员函数。
>
> `std::function`是C++标准库中的函数对象包装器，它可以用于存储和调用任意可调用对象，包括函数指针、成员函数指针、函数对象和lambda表达式等。`std::function`提供了一种通用的方式来表示和操作可调用对象，使得函数的类型可以在运行时确定。
>
> 区别：
>
> 灵活性：`std::function`可以存储任意可调用对象，而函数指针只能指向特定类型的函数或静态成员函数。
>
> 可变性：`std::function`的目标可调用对象可以在运行时动态更改，而函数指针一旦指向一个函数就无法更改。
>
> 复杂性：函数指针的语法相对简单，而`std::function`是一个模板类，使用时需要指定可调用对象的类型，代码稍微复杂一些。

线程和进程、协程区别

> 

buffer缓冲区

> 

线程池怎么实现得（手写线程池）(主要有哪些元素)

> 线程池组成：
>
> 1. 任务队列：用双端队列实现，能从尾部加入用户任务对应的可调用对象
> 2. 用户任务：封装了用户任务，包含任务函数和参数
> 3. 管理线程：用于管理工作的线程
> 4. 工作线程：执行回调函数
>
> 生产者--消费者模型
>
> 工作原理：首先创建并启动一组线程，称为线程池threads_，由用户指定其大小`maxQueueSize`_，每个元素对对应一个线程。每个线程函数都是一样的，在其中会运行一个loop循环：从双端队列取出一个任务对象`task`，如果非空，就执行之，如此往复。
> 当有一个用户线程想要通过线程池运行一个用户任务时，就可以将用户任务函数及参数封装成一个可调用对象Task f，然后通过线程池接口，将f加入双端队列末尾。当线程池有线程空闲时（未执行用户任务），就会从双端队列头部取出一个Task对象task，然后执行之。

单例模式

> 

定时器需要哪些属性

> 1. 定时器需要记录设置的超时时间
> 2. 定时器到期时，需要执行相应的回调
> 3. 如果有重复事件（心态机制），需要记录超时时间间隔
> 4. 标记是否为重复事件的bool变量
>
> 
>
> 用红黑树来管理定时器

muduo定时器模块的特点

> 1. 整个TimerQueue只使用一个timerfd来观察定时事件，并且每次重置timerfd时只需跟set中第一个节点比较即可，因为set本身是一个有序队列。
> 2. 整个定时器队列采用了muduo典型的事件分发机制，可以使的定时器的到期时间像fd一样在Loop线程中处理。
> 3. 之前Timestamp用于比较大小的重载方法在这里得到了很好的应用



定时器的实现

> sleep()、alarm()、usleep()在实现时有可能用了SIGALRM信号，在多线程程序中处理信号是个相当麻烦的事情，应当尽量避免。
> linux timerfd_create()把时间变成了一个文件描述符，该“文件”在定时器超时那一刻变得可读，这样就能很方便的融入select()、poll()框架中，用统一的方式来处理IO事件和超时事件。
> 对于不支持timerfd_create()的环境，可以考虑起一个线程来模拟timerfd功能，到期往特定描述符写入一定数据。这样可以达到统一管理文件描述符的方式来编码。

muduo线程唤醒方式

> 如果用户在当前IO线程调用这个函数，回调会同步执行，如果用户在其他线程调用runInLoop()，cb会被加入队列，IO线程会被唤醒来调用这个Functor。唤醒的方式一般有三种。
>
> 1. pipe，使用fd[0] 为读端，fd[1]为写端，半双工。等待线程关注fd[0]的可读事件。
> 2. socketpair，也有一对文件描述符，可用于双向通信，全双工。
> 3. eventfd。eventfd是一个比pipe更高效的线程间事件通知机制，一方面它比pipe少用一个pipe
>    descriptor，节省了资源。另一方面，eventfd的缓冲区管理也简单的多，全部“buffer"只有定长8bytes，
>    不想pipe那样可能有不定长的真正buffer

回调函数放在当前IO线程执行目的

> 可以不用锁的前提下，保证线程安全。

muduo的shutdown没直接关闭Tcp连接

> muduo的 TcpConnection没有提供close，而只提供shutdown()，这么做是为了**收发数据的完整性**。
>
> Tcp是一个全双工协议，同一个文件描述符即可读又可写，shutdownWrite() 关闭了 “ 写”方向的连接，保留了“ 读 ”方向，这称为TCP half-close。如果直接close(socket_fd)，那么socket_fd就不能读或写了。
>
> 用shutdown而不用close的效果是，如果对方已经发送了数据，这些数据还“ 在路上 ”，那么muduo不会漏收这些数据。换句话说，muduo在TCP这一层面解决了“当你打算关闭网络忘记的时候，如何得知对方是否发了一些数据而你还没有收到？”这一问题。当然，这个问题也可以在上面的协议层解决，双方协商好不再互发数据，就可以直接断开连接了。
>
> muduo把“主动关闭连接”这件事分成两步来做，如果要主动关闭连接，它先关闭本地的“写”端，等对方关闭之后，再关闭本地“读”端。另外如果当前output buffer里面还有数据尚未发出的话，muduo也不会立刻调用shutwownWrite，而是等到数据发送完毕再shutdown，可以避免对方漏收数据。

muduo什么时候真正close socket

> 在TcpConnection对象析构的时候，TcpConnection持有一个Socket对象，Socket是一个RAII handler，它的析构函数会close(sockfd_)。

rpc框架的实现日志怎么实现的，知不知道无锁队列

> 

muduo的定时器怎么实现的 ，线程池怎么工作的

> 

muduo库底层是怎么实现的

> 

问webserver和muduo的异同

> 

如果线程阻塞对服务性能的影响

> 线程阻塞会影响性能，因为当一个线程被阻塞时，它将无法执行其他任务，而其他线程也无法使用该线程所占用的资源，这会导致系统资源的浪费和系统的响应时间变慢。如果一个线程阻塞时间过长, 那么它的等待时间就会浪费掉，这会导致资源的利用率下降，从而影响系统的性能。此外，线程阻塞还会导致上下文切换的频繁发生，增加系统的开销，从而影响系统的性能。因此，在编写多线程程序时，应该尽量避免线程阻塞，或者使用**异步编程**模型来避免线程阻塞对性能的影响。

全局buffer加锁控制对性能的影响，如何解决

> 

应用层为什么需要Buffer

> 非阻塞IO的核心思想是避免阻塞在read()或write()或其它IO系统调用上，这样可以最大最大限度的复用
> thread-of-control，让一个线程服务于多个socket连接。IO线程只能阻塞在IO Multiplexing函数上，如select/poll/epoll_wait。这样一来，应用层的缓存是必须的。其实任何一个生产者消费者模型都会涉及到缓存问题，不同的业务需求设计出不同架构的缓存。
>
> TcpConnection需要有output buffer。假设程序想发送100KB的数据，但是调用write之后，操作系统只接受了80KB（受TCP advertised window控制），调用者肯定不想原地等待，如果有buffer，调用者只管将数据放入buffer，其它由网络库处理即可。
> TcpConnection需要有input buffer。TCP是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”和“一次收到两条消息的数据”等情况。如果有buffer，网络库收到数据之后，先放到input buffer，等构成一条完整的消息再通知程序的业务逻辑。

muduo中buffer的设计

> **Buffer::readFd 栈缓冲extrabuf的使用**
>
> - 一次读的数据越多越划算，可减少系统调用。
> - 减少内存使用。如果给每条连接一建立就分配固定大小的读写缓冲区，而
>   这些缓冲区的使用率很低，这样必然会浪费内存。
>
> **Buffer的prepend设计**
>
> buffer提供了`prependable`空间，让程序能以很低的代价在数据前面添加几个字节。比方说程序以固定的4个字节表示消息的长度，当序列化一个消息时，可以一直`append（）`直到序列化完成，然后再在序列化数据的前面添加消息的长度，这样可以简化客户端代码，以空间换时间。

**muduo的 buffer不足之处**

> Buffer的内存的迁移都是拷贝的。Buffer::makeSpace中涉及到的`std::vector::resize()`以及`std::copy()`都会进行内存的拷贝迁移。
>
> muduo的设计目标是用于开发公司内部的分布式程序，并不涉及到高并发、高吞吐量。muduo更注重的是易用性，对于那种千兆网卡吞吐量还未跑满，CPU已经饱和的情况是比较适合用的。
> muduo的Buffer设计是一个连接对应两个Buffer（一个Input Buffer，一个Output Buffer），对于**多路数的长连接也是不适用的**。
> 理想的Buffer应该是多个链接共用一个Buffer，而且最好是Zero Copy。

使用muduo库有遇到很多bug吗，C++调试技巧，有没有遇到什么困难？

> 

开发服务过程遇到了什么困难

> 

Redis和MySQL数据保证一致性还有其他方案吗？

> 

 Nginx处理并发的模式

> 

整体架构

> 

轮询派发

> 

多线程的任务队列如何处理的？

> 1. 使用互斥锁（mutex）和条件变量（condition variable）来保证对任务队列的访问是线程安全的。
> 2. 使用条件变量来实现任务队列的阻塞和唤醒机制。当任务队列为空时，消费者线程会阻塞等待；当有新任务到来时，生产者线程会唤醒等待的消费者线程。

如何改进呢

> 

先问线程池，问每个线程都在做什么

>不断调用`epoll_wait`获取当前`epoll`所管理的监听事件并处理

怎么分配的工作，如果当前线程管理的这些连接都比较活跃怎么保证高性能

> 如果是某一个线程的所有连接都比较活跃的话，可能这个线程中的连接就会大量堵塞。但是其他线程的不会收到印象，如果想要进一步提高性能。就必须重新去调度，将活跃的连接分配给空闲的线程。

怎么去重新调度

>  应该可以在服务器运行期间记录连接的活跃程度，然后重新分配叭。具体怎么做，不太清楚，没有在服务器中用到。

 那测试过这个服务器吗，真的高性能吗

> 用`webbench`和`apachebench`测了一下，大概4万左右吧，说`webbench`比较简单，不能测试复杂的应用场景。

**你还做了个定时器，拿他来关闭长连接，为什么要这么做**

> 防止资源泄漏：长时间保持不活跃的连接会占用系统资源（如文件描述符、内存等），如果不及时关闭，可能会导致资源耗尽。
>
> 提高并发能力：通过及时关闭不活跃的长连接，可以腾出资源服务更多的活跃连接，提高服务器的并发处理能力。
>
> 就是如果有连接长时间不活动，就关闭吗，减少服务负载。

**tcp连接不是有个保活机制吗，为什么你要自己做这个呢。**

> TCP保活需要操作系统配置，**默认配置通常不适用于所有应用**。例如，Linux上的**默认保活探测间隔为7200秒（2小时）**，这对于许多应用来说太长了。
>
> **应用层定时器可以精确控制超时时间和处理逻辑**。例如，可以设定30秒内无数据交互就关闭连接，而不需要等待TCP保活的超时。
>
> 在高并发服务器中，尤其是**处理大量短连接或实时通信的场景**下，应用层定时器可以显著提高系统的资源利用率和响应速度。例如，在一个聊天应用中，用户可能频繁上线和离线，应用层定时器可以确保快速清理不活跃的连接，保证系统能够及时响应新连接的请求。

\* **你这个双缓冲机制实现异步日志库是怎么实现的。**

> 服务端和日志端都有一个Buffer，服务端产生的日志不会直接写入日志文件里，而是先写入服务端的Buffer中，经过一段时间或者服务端Buffer满了之后，日志端就会交换Buffer A和Buffer B。然后写入日志文件里

\* **为什么不用单缓冲呢**

> 单缓冲区在高并发情况下容易被填满，需要频繁地写入磁盘，导致频繁的I/O操作，从而影响系统性能。
>
> 在写入磁盘的过程中，可能会阻塞日志的追加操作，导致性能下降
>
> 多线程同时写入单缓冲区时需要加锁，频繁的锁竞争会导致性能下降。
>
> 单缓冲的话，主要是在写入和写出时，要加锁。而双缓冲的话，加锁只有交换的时候。

muduo 的定时器是如何实现的

> 

 Nginx 的 TCP 负载均衡

> 

Redis 的发布订阅模式

> 

muduo网络库有什么改进的地方

> 

如何实现高并发的，IO 线程和业务线程为什么分开，压测过吗

> **提高响应速度**：
>
> - I/O 线程处理网络 I/O 操作，可以迅速响应和处理大量请求，减少请求的等待时间。
> - 业务线程专注于复杂的业务逻辑处理，不受 I/O 操作阻塞的影响。
>
> **资源利用最大化**：
>
> - I/O 操作通常是高并发低 CPU 使用率的，而业务逻辑处理通常是低并发高 CPU 使用率的。分开处理可以更好地利用 CPU 资源和 I/O 带宽。
>
> **简化代码逻辑**：
>
> - 分离 I/O 和业务逻辑，使代码更加清晰、易于维护和扩展。
>
> **分离 I/O 线程和业务线程**：
>
> - **I/O 线程**：专门处理网络 I/O 操作，如接收请求、发送响应等。
> - **业务线程**：专门处理业务逻辑，如数据处理、数据库操作等。
> - 这种分离可以使 I/O 操作和业务逻辑并行执行，提高系统的吞吐量

重构点在哪，thread 类底层如何创建线程，thread_local 了解吗

> 

我说了仿muduo库，面试官问你觉得muduo库有什么设计问题？

> `muduo` 网络库中的线程池依赖于 Thread 的实现

采用多线程对性能有多大提升

> 

使用多线程达到性能瓶颈后，有什么其他提升性能的方式

> 1. 减少锁竞争
> 2. 使用线程池
> 3. 使用代理服务器
> 4. 减少上下文切换

说一下`muduo`发一条消息的整个流程   

> 

你项目的提高并发是怎么做的？  

> 

提出来新需求要求实现，已读未读如何实现？未读数存在哪里？（mysql上） 说一下表结构大概会怎么设计？怎么判断他已读，以及如何整个的设计流程？  

> 

已读未读数定义成全局变量存放再内存中，如果服务器宕机了，数据不就丢失了吗？  

> 我的回答是因为存储了所有的消息，因此即使宕机了，我们只需统计一下我们的数据表的信息，重新把已读未读数统计出来。  

群聊的人数上限是多少？为什么只设置100的上限？  

>  

说一下你整个rpc项目怎么来调用的， 怎么实现的？

> 服务提供方先向`zookeeper`上注册服务
>
> 服务消费方以本地调用方式调用服务，stub接收到调用后将方法和参数等组织成能在网络中传输的消息体，找到服务地址，并将消息发送到服务端；服务端的stub接受普到消息后进行解码，根据解码结果调用本地方法，并将结果返回给server_stub，然后组装并发送给消费方，消费方得到最终结果 

![img](https://mzy777.oss-cn-hangzhou.aliyuncs.com/img/939e4b81aeb94e309a2768841e817ae1.png)

> ①服务端启动的时候先扫描，将服务名称及其对应的地址(ip+port)注册到注册中心，这样客户端才能根据服务名称找到对应的服务地址。
>
> ②客户端去注册中心找服务地址，有了服务地址之后，客户端就可以通过网络请求服务端了。
>
> ②客户端调用远程方法的时候，实际会通过创建代理对象来传输网络请求。
>
> ④客户端生成request对象（类名、方法名以及相关参数）作为消息体，拼接消息头，然后通过muduo传输过去。
>
> ⑤当客户端发起请求的时候，多台服务器都可以处理这个请求。通过负载均衡选一台服务器。
>
> ⑥服务器收到客户端传过来的信息后进行解码，看看客户端需要要干什么，然后反射调用方法得到result，把result封装成rpcMessage，再传回去。
>
> ⑦客户端收到rpcMessage，解码，拿到自己想要的结果；

`zookeeper`在你的项目中起到什么作用

> 注册中心：注册服务，发现服务
>
> 每个后台server 进程启动后，都会向zk 上去注册自己对外提供的rpc 服务；当调用rpc 方法的client 想要调用一个服务，会去zk 上查询下rpc 服务所在的server 节点，拉取服务所在server 的ip 地址和port 信息，然后进行远程服务调用。所在zk 在项目中主要做一些服务注册和服务发现。

  什么时候去把服务注册到zk上？我回答先把rpc服务注册到zk上之后才会打开我们的网络模块   

> 先把服务注册到zk上，再打开网络服务
>
> **服务发现**： 在分布式系统中，服务需要注册到 Zookeeper 以便其他服务能够发现并与之通信。如果网络模块在服务注册之前启动，其他服务可能无法找到该服务，导致通信失败。
>
> **一致性和可靠性**： 在注册成功之前，服务尚未正式对外提供。因此，先注册服务可以确保在服务正式对外提供之前，它已经被其他组件识别和记录。
>
> **初始化顺序**： 按照合理的初始化顺序，可以确保系统在启动过程中不会出现依赖问题。例如，网络模块依赖于服务注册，而服务注册依赖于 Zookeeper。

再问另外一台服务上线后怎么去zk上面去找的，如何知道这台服务？   

> 1. **启动服务并连接 Zookeeper**： 新服务启动时，需要首先连接到 Zookeeper 服务器。
>
> 2. **查找目标服务**： 在 Zookeeper 的特定路径（如 `/services`）下查找目标服务的节点。
>
> 3. **获取服务信息**： 读取目标服务节点的数据，获取服务的地址和端口信息。
>
> 4. **与目标服务通信**： 使用获取到的地址和端口信息，与目标服务建立通信。

rpc与http的区别？ 

> **HTTP** 协议，又叫做**超文本传输协议**。而 **RPC**，又叫做**远程过程调用**。它本身并不是一个具体的协议，而是一种**调用方式**。
>
> 服务发现：http是通过域名，使用dns服务解析得到背后ip地址；rpc一般有专门的中间服务存储服务名和ip
>
> http的body一般使用json进行序列化，而rpc定制化更高，可以使用protobuf
>
> **HTTP/2.0** 在 **HTTP/1.1** 的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。

  项目中碰到的难点？

> 

布隆过滤器？  

> 用于测试一个元素是否是集合中的成员。布隆过滤器可以用较少的空间来表示一个集合，但有一定的误判概率。

 一致性哈希 --说了很多面试官打断了我，说get到点了 不需要继续说了   

> 一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而**一致哈希算法是对 2^32 进行取模运算，是一个固定的值**。
>
> 一致性哈希算法就很好地解决了分布式系统在**扩容或者缩容**时，发生过多的数据迁移的问题。
>
> **在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响**。
>
> 但是**一致性哈希算法并不保证节点能够在哈希环上分布均匀**，这样就会带来一个问题，会有大量的请求集中在一个节点上。所以，**一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题**。
>
> **不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。**
>
> ![img](https://mzy777.oss-cn-hangzhou.aliyuncs.com/img/dbb57b8d6071d011d05eeadd93269e13.png)

 红黑树比平衡二叉树的优势？ 

> 红黑树并没有要求左右子树的高度差不超过1，红黑树的插入和删除操作相对简单一些，旋转次数也较少，因此在实际应用中性能更高。

项目中MQ用到了吗？了解kafka这些吗（不了解）  

> 

分布式锁如何实现？

> 1. 基于数据库实现分布式锁；
>
>    基于数据库的实现方式的核心思想是：在数据库中创建一个表，表中包含**方法名**等字段，并在**方法名字段上创建唯一索引**，想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。
>
> 2. 基于缓存（Redis等）实现分布式锁；
>
>    使用缓存来实现分布式锁的效率最高，加锁速度最快，因为`Redis`几乎都是纯内存操作，而基于数据库的方案和基于`Zookeeper`的方案都会涉及到磁盘文件IO，效率相对低下
>
> 3. 基于`Zookeeper`实现分布式锁；
>
>    `Zookeeper`一般用作配置中心，其实现分布式锁的原理和`Redis`类似，在`Zookeeper`中创建瞬时节点，利用节点不能重复创建的特性来保证排他性。

redis的过期机制？

> `redis`的过期策略 - `定期删除 + 惰性删除`
>
> 定期删除：`redis`默认每隔 `100ms `就`随机抽取`部分设置了过期时间的 key，检测这些 key 是否过期，如果过期了就将其删除。
>
> 惰性删除：惰性删除不是去主动删除，而是在你要获取某个 key 的时候，`redis`会先去检测一下这个 key 是否已经过期，如果没有过期则返回给你，如果已经过期了，那么`redis`会删除这个 key，不会返回给你。

**讲一讲muduo库中你觉得最巧妙的设计**

> 1. `muduo`网络库在设计中确实使用了无锁编程的方式来提高效率，并且通过线程局部存储 (`thread_local`) 来确保每个 `Channel` 由其所属的 `EventLoop` 执行。
> 2. `muduo`使用一个特殊的文件描述符（`wakeupFd`）来唤醒 `EventLoop` 所在线程，从而实现线程间的任务调度和通信。
> 3. `muduo`定时器采用`timerfd`把时间变成了一个文件描述符，该“文件”在定时器超时那一刻变得可读，这样就能很方便的融入select()、poll()框架中，用统一的方式来处理IO事件和超时事件。

**介绍项目：RPC框架**

> 我的项目是利用C++11实现的简易RPC网络通信框架，实现了服务的发布、注册、远程过程调用等功能，在这个项目中我使用`zookeeper`的watch机制和znode节点实现服务的的分布式部署，利用`protobuf`进行数据的序列化和反序列化，利用`muduo`进行高性能网络发送，利用生产者消费者模型编写异步日志模块

你为什么选择使用`zookeeper`作为服务注册中心

> 在分布式环境中，需要用到一个服务注册中心，来将发布的**服务注册**到上面，保证客户端在这个注册中心找到对应的节点，同时服务注册中心还要能够监听注册服务结点的变化，如何节点出现故障了就需要**动态删除**节点，同时也需要通知客户端节点节点发生了变化，需要做哪些处理，而`zookeeper`正好实现了这么一个机制，它通过**心跳机制**来检测`rpc`节点是否还存在，每隔一段时间`zookeeper`服务端就向`rpc`节点发送心跳包来判断`rpc`节点是否存在，如果不存在就删除节点，同时通过`watch`机制告诉客户端节点出现了变化，需要做出相应的变化。

你用没有考虑使用其它中间件做服务注册中心

> `zookeeper`刚好满足我的服务注册中心的要求，所以我就选了它

有了解过其它的吗？

> 消息队列、Kafka、ZeroMQ和RabbitMQ

做项目要横向对比，分析所有组件的优缺点，选出最合适的，不能说只看一两个

> 

你的项目使用`protobuf`进行数据序列化和反序列化，为什么

> 利用`protobuf`主要是为了进行数据的序列化和反序列化，也可以用`json，xml`，但是它们没有`protobuf`高效，`protobuf`是以二进制形式存储的，`json`是以`key value`形式存储的，`xml`存储需要额外的文本，所以在同等条件下，`protobuf`的数据大小较小，传输效率较高，而且`protobuf`支持`rpc`方法的描述，更加有利于实现`rpc`服务

了解`protobuf`的底层实现吗

> 额，只了解过它的基本使用场景，没有了解底层具体实现(感觉要寄了)

也就是你这个项目都是使用了其它库，没有深入理解过，这个项目那个部分你深入了解过

> `muduo`库部分，我在做这个项目的时候需要一个高性能网络传输框架，我就使用了`muduo`库，之后我看了`muduo`库的源码，进行了相应的了解

了解过源码，那么具体说说`muduo`的实现

> 使用`One Loop per thread`的设计模式，可以很好的处理网络IO和数据发送，不会导致我的框架在网络IO部分出现问题
>
> One Loop per thread:一个线程里面还有一个`EventLoop`，一个Reactor模型包含4个部分，Event事件，Recator反应堆，事件分发器，事件处理器；在muduo库设计中，我们主线程的`EventLoop`只负责新用户的连接，每次找到连接的用户之后就会把他交给子线程进行处理，而每个子线程里面也维护者一个`EventLoop`，它负责监听已连接用户的读写事件，如果一个文件描述符上发生了对应文件描述符感兴趣的事件，就调用对应的事件处理器`EventHandler`进行处理

为什么使用muduo库

> 很好的将网络模块和业务模块拆分开，耦合度低，`muduo`库刚好满足我的需求，而且它的效率也很高，所有我是用了它

其它很多库也可能使用了这个设计，甚至比它设计好，为什么使用了它

> `muduo`基于C++11进行实现，很好的支持了现代C++语法，使用bind进行回调时间的绑定，更有利于现代C++的使用，同时它的设计也非常棒，基于`one Loop per thread`，效率非常好，同时他对于C++新手非常友好，代码通俗易懂，而且一般的网络库线程之间都是通过加锁来实现线程间通信的，而`muduo`网络库采取无锁的方式，利用`threadlocal`来判断当前`Chanel`是否有该`EventLoop`执行，更加提升了效率，对于学习C++高性能服务器开发非常有帮助
>
> muduo 提供了简洁的接口，隐藏了许多底层复杂性，使得开发者可以专注于业务逻辑的实现，而不需要深入了解底层的网络编程细节

你是如何学习muduo库的，仅仅只是看了它的代码？

> 没有的，我在学习`muduo`库的时候不仅看过它的源代码，我还对muduo库进行改进，实现了muduo库的TcpServer部分，去除这部分对于`boost`库的依赖，同时进行归纳总结(埋坑了)

你是怎么进行归纳总结的，使用什么工具

> 我一般在本地使用`typora`进行归纳总结，然后自己一个人进行复习

项目部分差不多了，接下来问点算法的吧，如果求一个数的n次方

> 直接n个数相乘，然后考虑n为负数能特殊情况，判断特殊情况即可

还有其它想法吗，可以考虑优化一下吗？

> 递归快速幂法