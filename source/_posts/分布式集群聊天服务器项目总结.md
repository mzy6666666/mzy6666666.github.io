---
title: 分布式集群聊天服务器项目总结
date: 2024-07-08 16:25:02
tags: muduo 聊天服务器
categories:
    项目
password: mzy666
abstract: 有东西被加密了, 请输入密码查看.
message: 您好, 这里需要密码.
wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.
wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.
---

## 使用RPC框架将集群聊天服务器改为分布式聊天服务器

**项目介绍**

原项目是一个实时聊天服务器，使用重构的 muduo 网络库搭建网络核心模块，Nginx 实现聊天服务器的集群来提高并发能力，Redis 作为消息中间件，MySQL 作为数据存储，json 作为通信协议进行消息的序列化和反序列化，实现了网络层、应用层以及数据层的解耦，让开发者能够专注于业务模块的开发。现在，项目将改为使用自定义的 RPC 框架实现分布式集群架构，以进一步提高系统的扩展性和性能。

**具体工作**

**集群架构改进**：由简单的 Nginx 集群架构改为分布式集群架构，提升系统的扩展性和容错能力。

**服务发现与注册**：引入 Zookeeper 作为配置注册中心，实现服务的动态发现和注册，确保各个分布式节点之间的高效协同。

**RPC 框架**：引入自定义的 RPC 框架 mprpc，取代传统的 HTTP 请求，提升分布式通信的性能和稳定性。RPC 框架的引入使得服务间的调用更加高效，降低了系统的耦合度。

**负载均衡**：Nginx 仍作为前端负载均衡器，分发客户端请求到不同的 RPC 服务节点，提高系统的并发处理能力。

**序列化格式**：将通信协议的序列化格式由 JSON 改为 Protobuf，显著提高了数据传输效率，降低了序列化和反序列化的开销。

![image-20240625122317498](https://mzy777.oss-cn-hangzhou.aliyuncs.com/img/image-20240625122317498.png)

### 简单描述一下你的项目

> 项目是一个网络服务器项目，分为3个模块
>
> 首先，网络模块，采用的是muduo网络库来设计，该网络库的优点就是解耦了网络模块代码和业务模块代码，能够让开发者专注于业务的开发。
> 然后，业务层运用了一些C++11的一些技术，比如map、绑定器等，主要是做了一个消息ID以及这个消息发生后的一个回调操作的绑定，回调机制。当网络I/O给我吐出来一个消息请求的时候，根据请求解析出Json，得到消息ID，然后处理该消息。
> 数据存储层运用了MySQL，对项目上的一些关键数据进行了落地，比如说用户的账号、离线消息，群组消息等，都是在数据库存储。并实现了数据库连接池，省去大量三次握手四次挥手过程，提高对数据库的访问效率。
> 以上是单机服务器的基本模块，但单机服务器并发能力有限，考虑到快速提高项目的并发能力，所以部署多台服务器，就要配置Nginx TCP负载均衡（因为是长连接，所以是基于TCP的）。
>
> 各个服务器有不同的用户注册，那这些不同服务器的用户想要通信的话，项目里引入了redis的发布-订阅。
>
> 后面做了rpc的框架，考虑到我的服务器中不是所有的模块都需要在每台服务器上部署，把系统拆分成各个模块，需要高并发的模块就多放几台服务器，这样就进一步提高的服务器的性能



> 我在Linux下使用VScode进行项目开发，通过Cmake构建项目，并使用gdb进行调试。项目完成后，通过Linux shell输出编译脚本，并将项目托管到GitHub。
>
> 项目分为3个模块：
>
> 1. **网络模块**：我深入剖析了muduo网络库的核心组件，用C++11重写了这些组件，去除了对boost库的依赖，实现了一套网络库。这解耦了网络模块和业务层代码，使开发者能够专注于业务开发。
> 2. **业务模块**：我使用C++11的技术（如map、绑定器、函数对象）实现了消息ID与回调操作的绑定机制。当网络I/O通知服务器有消息请求时，服务器会处理读事件的回调操作（OnMessage）。通过解析消息中的JSON，获取消息ID，并通过回调处理消息。
> 3. **数据存储层**：我使用MySQL存储关键数据（如用户ID、离线消息、好友列表、群组列表）。我还实现了一个MySQL数据库连接池，并通过model类封装数据库操作，提高了访问效率。
>
> 在单机模式下，项目包含以上模块。但为了提高并发能力，我支持多机扩展，部署多台网络服务器，并在前端使用Nginx进行负载均衡。项目基于TCP的私有协议，采用C/S模型进行通信，因此配置了Nginx的TCP负载均衡，建立长连接，以支持消息推送。
>
> 由于不同服务器上的注册用户需要通信，我引入了Redis作为消息队列，利用其发布/订阅功能实现跨服务器的消息通信。
>
> 在后来，我设计了一个RPC框架，并考虑到聊天服务器不是所有模块都需要部署在每台服务器上。将系统拆分成各个模块，通过这种拆分，对需要高并发的模块部署更多服务器，从而进一步提高系统性能。

### 分布式集群聊天服务器项目总结

> 首先，我是在Linux下使用VScode进行项目的开发，通过Cmake构建项目，使用gdb进行项目问题的调试，项目完成后通过Linux shell输出项目编译脚本，并把项目托管到我的GitHub上。
> 这个项目分为3个模块，首先，第一个模块是网络模块，我通过深入剖析muduo网络库的核心组件，采用C++11重写muduo库的核心组件，将依赖于boost库的技术点转化为C++11的知识点，使其脱离boost库，实现出一套网络库来使用，使用这个网络库的好处是：解耦了网络模块的代码和业务层的代码，能够让开发者专注于业务层的开发。然后第二个模块是业务模块，也就是是服务层，我使用了C++11的一些技术，比如说，map，绑定器，函数对象，做了一个消息id和这个消息发生以后的回调操作的绑定，相当于是做了一个回调机制，当这个网络I/O给服务器通知有消息请求，服务器处理客户注册的读事件的回调操作(OnMessage)，通过消息请求，从消息里面(buffer)解析出这个json，得到消息id，通过回调操作，就可以处理这个消息了。第三个是数据存储层，我使用的是MySQL，对于项目的关键的数据进行存储，比如说，用户的id，离线消息，好友列表，群组列表的关系，都是在MySQL存储。我还通过实现一个MySQL数据库连接池，在项目代码上还通过书写model类对业务层封装底层数据库，提高MySQL数据库的访问效率。
> 在单机模式下，主要就是这3个模块，但是单机的并发能力是有限的，所以，我考虑项目的整体的并发能力，让项目可以支持多机的扩展，部署多台网络服务器，前面需要挂1个Nginx负载均衡，这个项目是基于TCP的私有协议，自己去搭建的基于C/S模型的通信，所以我通过对Nginx的tcp的负载均衡配置，做一个长连接，因为是消息聊天通信，客户端不仅仅要给服务器主动发消息，而且服务器还要主动给客户端推消息，所以必须是长连接，短连接做不到这个功能，因为短连接没有办法给客户端推消息，另外，在负载均衡里面，因为是各个服务器有不同的人进行注册，不同服务器上注册的用户需要进行通信的话，我在这里引入了Redis作为一个MQ消息队列的功能，利用它的发布订阅功能实现了跨服务器的消息通信功能。

### 负载均衡是什么

> 假设一台机器支持两万的并发量，现在我们需要保证八万的并发量。首先想到的是升级服务器的配置，比如提高 CPU 执行频率，加大内存等提高机器的物理性能来解决此问题，但是单台机器的性能毕竟是有限的。这个时候我们就可以增加服务器的数量，将用户请求分发到不同的服务器上分担压力，这就是负载均衡。那我们就需要有一个第三方组件充当负载均衡器，由它负责将不同的请求分发到不同的服务器上。

### 选择Nginx的tcp负载均衡模块的原因：

> 1. 把client的请求按照负载算法分发到具体的业务服务器ChatServer上
> 2. 能够ChantServer保持心跳机制，检测ChatServer故障
> 3. 能够发现新添加的ChatServer设备，方便扩展服务器数量

### redis发布-订阅功能解决跨服务器通信问题

> 之前的ChatServer是维护了一个连接的用户表，每次向别的用户发消息都会从用户表中查看对端用户是否在线。然后再判断是直接发送，还是转为离线消息。
>
> 现在集群服务器，有多个服务器维护用户。ChatServerA要聊天的对象在ChatServerB，ChatServerA在自己服务器的用户表中找不到。那么可能对端用户在线，它却给对端用户发送了离线消息。因此，需要保证跨服务器间的通信！
>
> 1. 那如何实现，非常直观的想法，可以让后端的服务器之间互相连接。
>
>    让各个ChatServer服务器互相之间直接建立TCP连接进行通信，相当于在服务器网络之间进行广播。这样的设计使得各个服务器之间耦合度太高，不利于系统扩展，并且会占用系统大量的socket资源，各服务器之间的带宽压力很大，不能够节省资源给更多的客户端提供服务，因此绝对不是一个好的设计。
>
> 2. 最好的方式就是引入中间件**消息队列**，解耦各个服务器，使整个系统松耦合，提高服务器的响应能力，节省服务器的带宽资源

### 为什么一般集群都要用3台服务器来组建,不用两台

因为三台服务器可以提供更高的可靠性、可用性和容错能力。

> 1. 容错
>
>    **多数决策机制**: 当集群使用三台服务器时，可以实现**多数决策机制**（majority voting）。即使一台服务器发生故障，剩余的两台服务器可以继续协同工作，确保系统的可用性和一致性。
>
> 2. **Paxos和Raft等共识算法**: 许多分布式系统使用 Paxos 或 Raft 等共识算法来保证数据的一致性和正确性。三台服务器是这些算法的最小配置，可以确保在存在少数节点故障的情况下，系统仍然能够达成一致。
>
> 3. **避免脑裂（Split-Brain）问题**: 在两台服务器的情况下，如果两台服务器之间的连接出现问题（例如网络分区），系统无法确定哪一方是正确的，从而可能导致数据不一致的问题。三台服务器则能通过多数决策机制避免这种情况。

### 服务器架构

> 整个聊天服务器整体设计抽象为由多台服务器组合而成的一台超级性能的服务器，这些服务器形成一个小集合，部署一整套对外的服务。`set`模型弥补了单机能力的不足，对业务组合搭配成一个单元。本质上是对服务的一个高内聚的封装。

### 服务器中的抽象服务单元

> `ProxyService`作为整个服务单元的入口，**整个服务单元对外暴露的也是它的`Host`信息**，对于客户端的请求信息，它会首先去判断这个信息是哪个业务，如果是它绑定的业务，它就会去`Zookeeper`注册中心，找到提供这个服务的服务节点，并把这个请求信息分配给它。

### 你这里数据都是明文传输，不安全怎么解决？

> 进行加密
>
> **对称**加密码算法：加解密**效率**高，**AES**加解密算法
>
> **非对称**加密：公钥和私钥，加密复杂，效率慢，但是安全、RSA算法

### 客户端消息如何按序显示

> 消息添加序列号seq，接收方维护一个下一次应该接收消息的序列号，如果后发的提前到了，则会将消息缓存起来。加序号不仅可以保证消息按序到达，还可实现其他功能，如消息撤回
>
> 如果给消息添加一个时间戳，到达客户端再按时间排序，但是有问题，比如以1s为周期进行消息显示，很可能最先发的消息没有和它相邻的消息一起进行排序，即无法实现全局有序显示。

### 不能用短链接吗？

> 集群聊天服务器需要处理大量的客户端连接请求，而使用短链接会导致频繁的连接和断开操作，增加服务器的负担。当客户端需要发送消息时，短链接需要重新建立连接，而在连接的建立和断开时会产生额外的网络开销和延迟，影响系统的性能。

### 为什么要用redis作为跨服务器通信的组件，为什么各个server不能相互直接通信呢？

> 若各个server**直连**，则服务器还要承担客户端职责，服务器间**耦合性**高。需要和每个server相互连接，还要不断**心跳**。

### 问题1：MySQL远程连接失败

> 在用数据库连接的时候发现只有 `127.0.0.1` 能连接上，其他公网地址：`42.192.129.38`或者私网地址：`172.17.0.2`
>
> 1. 先去数据库查看 `mysql.user`下是的`host `是否是`localhost`，如果是，改成%（代码任意ip）



### 问题2：客户端中序列化的数据，经过`byte[]` 发送给C++服务端，服务端这边是能打印出收到的数据，但是反序列化失败，一直为空。

> 根据日志打印发现是`protobuf`解析出错，但是两端打印的数据都正常，尝试了一下把原来的`ParseFromString`改为`protobuf`的 `ParseFromArray`，发现**成功**了。但是后面看了`ParseFromString`的源码发现调用的还是 `ParseFromArray`，最后找到是我从`muduo`缓冲区中把所有数据打印了一遍，然后又从缓冲区里拿数据，此时缓冲区为空了，是没有数据的，肯定会反序列化失败。

### 问题3：portobuf 在 rpc 通信中 response 为nullptr

> 在运行过程中，`zookeeper`服务端可以正常连接，然后就报段错误了
>
> 用 `gdb `调试，发现报了错误
>
> 对于`response`来说，不能传空值，必须传一个对象，如果没有返回值，就构造`google::protobuf::Empty`对象。

### 项目缺陷

> - 对于数据库，包括mysql Redis部分，其并没有考虑高并发和高可用，也没有进行分库分表等设计。
>
> - 对于Nginx来说，这里的设计不是很好，其会承担很高的网络I/O压力。其实更应该去自己实现以下一个服务单元的注册中心，里面存储了服务单元的Host信息及负载压力等信息，然后客户端去主动拉取服务单元的信息，用哈希或者最小负载等信息去自己连接一个服务单元
> - 对于某个Service去选择具体的服务节点Server，这里博主采用的是最简单的轮询方法。但是我的Server都是动态上限的，如果Server 1上线工作了很久，处理的请求达到了3w，这个时候Server 2上线了，那么我的Service去分派请求的时候，就应该去侧重Server 2，而不是这个时候还要轮询。这个选择应该是最小负载，而不是轮询
> - 系统的容灾设计考虑还是不够全面，对于Server掉线，这方面可以用集群保证。但是对于Service节点掉线，其意味着整个具体业务服务不可用。另外，对于ProxyServiec掉线，将会导致整个服务单元不可用



# 